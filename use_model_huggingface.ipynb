{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c79c12",
   "metadata": {},
   "source": [
    "# 1.Hugging face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75363a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "    \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import BitsAndBytesConfig \n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd294191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [18:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Data processing error: CAS service error : ReqwestMiddleware Error: Request failed after 5 retries",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m nf4_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m      3\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# bnb_4bit_use_double_quant=True,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     bnb_4bit_compute_type\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnf4_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    605\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    606\u001b[0m     )\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\transformers\\modeling_utils.py:4900\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4891\u001b[0m     gguf_file\n\u001b[0;32m   4892\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4893\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[0;32m   4894\u001b[0m ):\n\u001b[0;32m   4895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   4896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4898\u001b[0m     )\n\u001b[1;32m-> 4900\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4902\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4907\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4920\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4921\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\transformers\\modeling_utils.py:1200\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[1;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[0;32m   1198\u001b[0m sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m-> 1200\u001b[0m     checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1214\u001b[0m     checkpoint_files \u001b[38;5;241m=\u001b[39m [resolved_archive_file] \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\transformers\\utils\\hub.py:1084\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[1;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m shard_filenames, sharded_metadata\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;66;03m# or download the files\u001b[39;00m\n\u001b[1;32m-> 1084\u001b[0m cached_filenames \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshard_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached_filenames, sharded_metadata\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\transformers\\utils\\hub.py:567\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;66;03m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001b[39;00m\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;66;03m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001b[39;00m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n\u001b[1;32m--> 567\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    569\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    570\u001b[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[0;32m    571\u001b[0m ]\n\u001b[0;32m    572\u001b[0m \u001b[38;5;66;03m# If there are any missing file and the flag is active, raise\u001b[39;00m\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\transformers\\utils\\hub.py:494\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         hf_hub_download(\n\u001b[0;32m    480\u001b[0m             path_or_repo_id,\n\u001b[0;32m    481\u001b[0m             filenames[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    491\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    492\u001b[0m         )\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 494\u001b[0m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\huggingface_hub\\_snapshot_download.py:332\u001b[0m, in \u001b[0;36msnapshot_download\u001b[1;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[0;32m    330\u001b[0m         _inner_hf_hub_download(file)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[0;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(local_dir))\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\tqdm\\contrib\\concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[1;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _executor_map(ThreadPoolExecutor, fn, \u001b[38;5;241m*\u001b[39miterables, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\tqdm\\contrib\\concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[1;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[0;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:608\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 608\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\huggingface_hub\\_snapshot_download.py:306\u001b[0m, in \u001b[0;36msnapshot_download.<locals>._inner_hf_hub_download\u001b[1;34m(repo_file)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_inner_hf_hub_download\u001b[39m(repo_file: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:1168\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1168\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1181\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:1720\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[0;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[0;32m   1719\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1720\u001b[0m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n",
      "File \u001b[1;32md:\\RAG\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:626\u001b[0m, in \u001b[0;36mxet_get\u001b[1;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m    624\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[1;32m--> 626\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Data processing error: CAS service error : ReqwestMiddleware Error: Request failed after 5 retries"
     ]
    }
   ],
   "source": [
    "model_name = \"qwen/Q\"  # Thay doi ten model o day neu can\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_type=torch.float16,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec672ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_new_tokens = 512\n",
    "\n",
    "model_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13cde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NTCUONG\\AppData\\Local\\Temp\\ipykernel_8372\\3975455254.py:5: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=model_pipeline,\n"
     ]
    }
   ],
   "source": [
    "gen_kwargs = {\n",
    "    \"temperature\": 0.1\n",
    "}\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=model_pipeline,\n",
    "                         model_kwargs=gen_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc575f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = llm.invoke(\"Explain the theory of relativity in simple terms.\")\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030233d",
   "metadata": {},
   "source": [
    "# Prompt Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8109d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# promt_template = PromptTemplate.from_template(\"\"\"INSTRUCTION: {prompt} \\n\\n RESPONSE:\"\"\")\n",
    "# user_prompt = \"How does a rocket work?\"\n",
    "# prompt = promt_template.format(prompt=user_prompt)\n",
    "# llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d20ae7",
   "metadata": {},
   "source": [
    "# Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbb912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# chat_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful assistant.\"),\n",
    "#     (\"user\", \"{prompt}\"),\n",
    "# ])\n",
    "\n",
    "# message = \"What is the capital of France?\"\n",
    "# chat_template = chat_prompt.format_messages(prompt=message)\n",
    "# chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d65b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a85579",
   "metadata": {},
   "source": [
    "# LLM chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ec343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = PromptTemplate.from_template(\"INSTRUCTION: {prompt} \\n\\n RESPONSE:\")\n",
    "\n",
    "# chain = prompt|llm\n",
    "# output = chain.invoke({\"prompt\": \"Explain the theory of relativity in simple terms.\"})\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79095e4b",
   "metadata": {},
   "source": [
    "# Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.output_parsers import JsonOutputParser\n",
    "# from pydantic import BaseModel, Field\n",
    "\n",
    "# class Joke(BaseModel):\n",
    "#     setup:str = Field(description=\"Question to set up the joke\")\n",
    "#     punchline:str = Field(description=\"Answer to resolve the joke\")\n",
    "\n",
    "# parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template = \"Answer the user query. \\n{format_instructions}\\nUser Query: {query}\\n\",\n",
    "#     input_variables=[\"query\"],\n",
    "#     partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "# )\n",
    "\n",
    "# chain = prompt | llm | parser\n",
    "# joker_query = \"Tell me a funny joke.\"\n",
    "# output = chain.invoke({\"query\": joker_query})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5401d93",
   "metadata": {},
   "source": [
    "# Load from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"data/paper_3750.pdf\")\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55bed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='VRAE: Vertical Residual Autoencoder for License\\nPlate Denoising and Deblurring\\nCuong Nguyen1, Dung T. Tran1,2, Hong Nguyen3,\\nXuan-Vu Phan1, Nam-Phong Nguyen1*\\n1Ha Noi University of Science and Technology, Ha Noi, Vietnam.\\n2Center of Environmental Intelligence, VinUniversity, Hanoi, Vietnam.\\n3University of Southern California, Los Angeles, United States.\\n*Corresponding author(s). E-mail(s): phong.nguyennam@hust.edu.vn;\\nContributing authors: cuong.nt227090@sis.hust.edu.vn;\\ndung.tt2@vinuni.edu.vn; hongn@usc.edu; vu.phanxuan@hust.edu.vn;\\nAbstract\\nIn real-world traffic surveillance, vehicle images captured under adverse weather,\\npoor lighting, or high-speed motion often suffer from severe noise and blur. Such\\ndegradations significantly reduce the accuracy of license plate recognition sys-\\ntems, especially when the plate occupies only a small region within the full\\nvehicle image. Restoring these degraded images a fast realtime manner is thus\\na crucial pre-processing step to enhance recognition performance. In this work,\\nwe propose a Vertical Residual Autoencoder (VRAE) architecture designed for\\nthe image enhancement task in traffic surveillance. The method incorporates an\\nenhancement strategy that employs an auxiliary block, which injects input-aware\\nfeatures at each encoding stage to guide the representation learning process,\\nenabling better general information preservation throughout the network com-\\npared to conventional autoencoders. Experiments on a vehicle image dataset\\nwith visible license plates demonstrate that our method consistently outperforms\\nAutoencoder (AE), Generative Adversarial Network (GAN), and Flow-Based\\n(FB) approaches. Compared with AE at the same depth, it improves PSNR by\\nabout 20%, reduces NMSE by around 50%, and enhances SSIM by 1%, while\\nrequiring only a marginal increase of roughly 1% in parameters.\\nKeywords: License Plate Deblurring, License Plate Denoising, Lightweight,\\nGeneralize Preservation\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='1 Introdution & Related Work\\nIn recent years, traffic scene understanding has become essential for intelligent trans-\\nportation, autonomous driving, and surveillance. However, real-world traffic images\\nare often degraded, reducing system reliability. Motion blur frequently occurs due to\\nfast vehiclecamera motion and low-cost sensors with limited exposure, while advanced\\nhigh-speed cameras remain costly [13]. Moreover, low-light conditions further intro-\\nduce noise, glare, and low contrast, particularly in Southeast Asian cities with dense\\nnighttime traffic and mixed lighting [46]. In addition, sensor limitations such as noise\\nand dynamic range exacerbate the trade-off between blur and signal strength [7, 8].\\nAs a result, these degradations severely impair downstream tasks like detection, recog-\\nnition, and segmentation [9, 10]. To address this, image enhancement techniques,\\nespecially including denoiser and deblurrer, are widely studied. While traditionally\\ntreated separately, traffic images often suffer from both simultaneously. Deep learn-\\ning methods have been explored including AE for compact representations, GAN for\\nenhanced perceptual quality, and FB for explicit distribution learning that balances\\nfidelity and diversity.\\nTraditional methods using AE have been widely adopted in such tasks due to their\\nability to learn compact latent representations and reconstruct images by filtering noise\\nfrom signal [11]. Classical convolutional AE have shown promising results in removing\\nadditive noise, as demonstrated in DnCNN [7], which employed residual learning and\\nbatch normalization for Gaussian denoising. Similarly, convolutional encoder-decoder\\nstructures have been successfully applied in deblurring, such as DeblurGAN [2], which\\ncombined adversarial training with perceptual loss to recover sharp textures. This facil-\\nitates direct mapping from corrupted inputs to clean outputs, effectively addressing\\ncomplex or spatially varying degradations. State-of-the-art deep generative methods,\\nsuch as GAN and Diffusion, have been integrated into restoration pipelines. Specifi-\\ncally, DeblurGAN-v2 [12] introduced a multi-scale GAN framework that achieved both\\nsharpness and stability in deblurring. Recently, FB models have emerged as a princi-\\npled alternative for image restoration, capable of learning invertible mappings between\\ndegraded and clean domains while preserving density information [13].Notably, FB\\nhas been introduced as a scalable and stable training framework for conditional image\\ngeneration, with its application to restoration explored in tasks demanding spatial\\nconsistency and smooth transitions, such as PnP-Flow [14, 15].\\nResearch Gap: AE have been employed in traffic image enhancement due to\\ntheir lightweight design, which is suitable for resource-constrained monitoring sys-\\ntems [1618]. However, shallow encoders typically capture only low-level features and\\nfail to recover mid- and high-level structures that are critical for fine detail restora-\\ntion [19, 20]. GAN improve perceptual realism with sharper edges and more natural\\ntextures, but often distort pixel-level fidelity, hallucinate numbers, limiting their reli-\\nability for traffic applications where accurate recovery of details such as license plates\\nand road markings is essential [21, 22]. FB models preserve pixel-level accuracy by\\noptimizing explicit likelihood functions, yet their high computational complexity and\\nmemory consumption make them impractical for real-time deployment on edge devices\\n[13, 23]. Therefore, a clear research gap exists in developing architectures that balance\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content=\"computational efficiency with accurate detail preservation for traffic image enhance-\\nment. For such reasons, the main contributions of this work can be summarized as\\nfollows:\\n We propose the Vertical Residual Autoencoder (VRAE)with an auxiliary\\nblock that vertically aggregates and refines embeddings for enhanced feature repre-\\nsentation. Experimental results show that VRAE achieves impressive improvements\\nacross PSNR, SSIM, and NMSE, while maintaining competitive performance even\\nwith reduced model parameters.\\n Through extensive experiments, we show that our approach not only encourages\\ngeneral information preservation in the early encoder layers but also outperforms\\nconventional autoencoders in terms of image compression, as evaluated by entropy\\nchange\\nx x  \\nResnet50 stage \\nMain block Auxiliary block\\nDecoder\\nX 1 x 3 x 4 x 5 X 2 \\nX' 1 X' 2 x' 3 x' 4 \\nE 1 E 2 E 3 E 4 E 5 \\nE 1 ' \\nE 2 ' E 3 ' E 4 ' \\nD \\nAuxiliary encoder Addition\\nNext\\nFig. 1 VRAE aggregates the input embedding by passing it through the feature embedding block\\nbefore forwarding it to the next encoder block.\\n2 Vertical Residual Autoencoder\\nProblem statementIn traffic surveillance systems, vehicle images are often degraded\\nby motion blur, sensor noise or adverse weather conditions, which obscure important\\nvisual details such as license plates, headlights, and road markings. Assume that all\\ndistortion is created by   N(0, 1) and the task of vehicle image enhancement is\\ndefined as learning a one-to-one mapping F: x  x from a degraded input image\\nx  R3WH to an enhanced output image x  R3WH, where x = x + .\\n2.1 Overall Architecture\\nVRAE comprises three components: (i) a parallel stack ofAuxiliary Encoders{Ei}N\\ni=1,\\neach extracting hierarchical features directly from the inputx; (ii) a continuous stack of\\nMain Encoders{E\\ni}N1\\ni=1 that further transforms the intermediate features and embeds\\nthem for residual injection into deeper stages; and (iii) a Decoder D that aggregates\\nfeatures from both streams to reconstruct the output x. The two streams interact via\\nelement-wise additions at multiple stages, enabling vertical residual learning.\\n3\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='2.2 Auxiliary Block\\nThe auxiliary path {E\\ni} acts as a feature embedding modulethat repeatedly extracts\\ncomplementary, input-conditioned features to be fused with the main stream. Each\\nblock follows a shallow ConvNormActivation structure and optionally includes\\npooling to summarize global context:\\nx\\ni = E\\ni(x), i = 1, . . . ,5. (1)\\nAs illustrated in Fig. 1, the auxiliary block operates in parallel with the main\\nencoder and generates an additional feature hierarchy {x\\ni} from the degraded input\\nx. These features are progressively injected into the corresponding layers of the main\\nblock through element-wise fusion, allowing the model to enrich low- and mid-level\\nrepresentations with complementary cues.\\n2.3 Main Block\\nIn our implementation, each main encoder Ei is instantiated as the i-th stage of the\\nResNet-50 architecture [24]. ResNet-50 employs a bottleneck design consisting of three\\nconvolutional layers: a 1  1 convolution for dimensionality reduction, a 3  3 convo-\\nlution for spatial processing, and another 1  1 convolution for restoring dimensions.\\nA skip connection adds the input to the output of the block, enabling better gradi-\\nent flow and alleviating the vanishing gradient problem. To ensure a fair comparison,\\nthe same ResNet-50based encoder backbone is also adopted for the baseline Autoen-\\ncoder (AE), GAN, and FB models. This guarantees that the observed performance\\nimprovement arises from the proposed vertical residual aggregation mechanism rather\\nthan architectural capacity differences.\\nAt each stage, the feature maps from the main and auxiliary encoder paths are\\nfused by element-wise addition before being passed into Ei. Formally:\\nxi = Ei\\n\\x00\\nxi1 + x\\ni1\\n\\x01\\n, i = 2, . . . ,5, (2)\\nwhere xi1 is the output of the (i1)-th main encoder block and x\\ni1 is the output of\\nthe (i1)-th auxiliary encoder block. This additive fusion serves as an embedding-level\\nintegration mechanism: it incorporates complementary information without increas-\\ning feature dimensionality, offering a parameter-efficient alternative to concatenation.\\nSuch design, inspired by residual networks [24] and multi-branch models [25], not only\\nstabilizes gradient flow but also enhances representational power, allowing the main\\nencoder to capture hierarchical abstractions while continuously integrating localized\\ncues from the auxiliary encoder.\\n2.4 Decoder\\nThe decoder D reconstructs x from xN using a cascade of transposed convolu-\\ntional layersfollowed by ReLU activations. Each transposed convolution progressively\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='upsamples the feature maps, restoring the spatial resolution:\\nyk = ReLU(ConvTranspose2dk(yk1)) , k = 1, . . . ,5, y0 = xN, yK = x. (3)\\nWe train the network with the mean squared error (MSE)loss only:\\nLMSE = 1\\nBCHW\\nBX\\nb=1\\nCX\\nc=1\\nHX\\ni=1\\nWX\\nj=1\\n\\x00\\nxb,c,i,j  xb,c,i,j\\n\\x012\\n, (4)\\nwhich averages the squared reconstruction error over the entire batch and all\\npixels/channels.\\n3 Experiments\\n3.1 Dataset Preparation\\nFrom the CCPD dataset [26], 3,036 high-resolution vehicle images were collected and\\nresized to 256256 pixels. The dataset was split into training, validation, and test sets\\nin a 70%15%15% ratio. Data augmentation, including rotations, was applied only to\\nthe training set, increasing it to 7,000 images. Low-resolution inputs were generated\\nby adding discrete noise (values in [0 , 9] scaled by 0.1) and applying a 3  3 average\\npooling filter iteratively 10 times, simulating low-quality surveillance footage. This\\ndegradation process is not identical but bears similarity to the synthetic degradations\\nused in SRMD [20], where combinations of blurring and noise are employed to simulate\\nmultiple low-resolution conditions.\\n3.2 Baseline choices\\nIn vehicle image restoration, GAN have been widely applied to inpainting, deblurring,\\nsuper-resolution, and artifact removal, achieving visually plausible and high-quality\\nresults [12, 21, 27]. In contrast, flow-based models such as RealNVP [28] and Glow [13]\\nprovide exact invertibility between image and latent spaces, enabling better preser-\\nvation of fine-grained details. Although still underexplored in this domain, these\\ncharacteristics make flow-based approaches particularly relevant to vehicle image\\nrestoration. To ensure a fair and controlled comparison, in this work we select both\\nGAN and FB methods as baselines and design them with main encoder and decoder\\nblocks consistent with our proposed method.\\n3.3 Settings\\nAll models were trained using the Adam optimizer with an initial learning rate of\\n1104 and a batch size of 16. Each model was trained for 100 epochs on an NVIDIA\\nRTX 4070 GPU.\\n AE, FB, VRAE: These methods utilize the MSE loss between the predicted and\\nground-truth HR images\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Table 1 Quantitative comparison of GAN, FB, AE (25 sub encoders), and the proposed\\nVRAE (25 sub-encoders). All metrics are evaluated on the test setand averaged across\\nall samples. The best results are in bold, and the second-best are underlined.\\nModel PSNR ( ) NMSE ( ) SSIM ( ) Params FPS\\nGAN (Gen) 28.743 0.006 0.842 14.59M 188\\nFB (Gen) 27.093 0.008 0.831 25.87M 35\\nAE2 27.787 0.007 0.840 0.375M1 411\\nAE3 26.159 0.011 0.802 2.77M 289\\nAE4 29.404 0.005 0.864 14.59M 189\\nAE5 27.243 0.008 0.793 48.43M 119\\nVRAE2 30.319 0.004 0.884 0.376M 399\\nVRAE3 31.052 0.003 0.898 2.78M 194\\nVRAE4 30.246 0.004 0.880 14.61M 90\\nVRAE5 30.032 0.003 0.860 48.48M 45\\n1M denotes millions, the unit of the number of parameters.\\n GAN: The GAN generator is trained with a hybrid loss combining pixel-wise MSE\\nand adversarial BCE. While MSE ensures closeness to the ground truth, it often\\nleads to overly smooth outputs. The adversarial loss encourages perceptual realism\\nby pushing the generator to fool the discriminator. The overall generator loss is thus:\\nLGEN = LMSE +   LBCE\\nwhere  = 10 3 is a small scalar balancing the adversarial loss contribution,\\nfollowing the original setting proposed in [21].\\n4 Result\\n4.1 Comparison with State-of-the-Art\\nTable 1 presents the quantitative comparison among GAN, FB, AE (25 stacks),\\nand the proposed VRAE (25 sub-encoders). Conventional GAN-based and FB-based\\napproaches show moderate perceptual quality but achieve relatively lower PSNR\\nand SSIM values compared to AE-based and VRAE-based models. Although stacked\\nAutoencoders (e.g., AE4) can provide competitive results (PSNR = 29.404 dB, SSIM\\n= 0.864), their performance is inconsistent across different stack depths, and some con-\\nfigurations (e.g., AE3 and AE5) degrade significantly. In contrast, the proposed VRAE\\nconsistently outperforms both AE and GAN variants in terms of reconstruction qual-\\nity. Notably, VRAE3 achieves the highest PSNR (31.052 dB) and SSIM (0.898), while\\nalso maintaining a low NMSE (0.003). VRAE2 also delivers strong results (PSNR =\\n30.319 dB, SSIM = 0.884), ranking second overall. Despite slightly higher parameter\\ncounts than AE2, VRAE models demonstrate a favorable trade-off between accuracy\\nand efficiency, with FPS values remaining competitive (e.g., VRAE2 at 399 FPS versus\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='Fig. 2 Average entropy change across encoder layers of AE and VRAE. In the proposed architecture,\\nentropy is computed from synthesized embeddings aggregated across encoder stages, capturing both\\nlocal and global contextual dependencies.\\nAE2 at 411 FPS). When compared with AE at the same stack depth, VRAE improves\\nPSNR by approximately 20%, reduces NMSE by around 50%, and enhances SSIM by\\nabout 1%, while introducing only a marginal increase of roughly 1% in the number\\nof parameters. These results indicate that the VRAE architecture not only stabilizes\\nperformance across different depths but also surpasses conventional baselines in both\\nreconstruction fidelity and computational efficiency.\\n4.2 Informative content preservation\\nAccording to [29], encouraging entropy preservation in the early layers promotes better\\ngeneralization. This is consistent with our hypothesis Figure 4.2, where we observe\\nthat models preserving higher entropy in the first encoder layers tend to achieve better\\nstability across subsequent layers. We first define the feature maps at encoder layerl as\\nFl  RClHlWl , (5)\\nwhere Cl is the number of channels, and Hl, Wl are the spatial height and width of\\nthe feature maps. Formally, the entropy change between consecutive layers l and l + 1\\nis defined as\\nHl = H(Fl+1)  H(Fl), (6)\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='where H(Fl) denotes the Shannon entropy of the feature maps Fl. The Shannon\\nentropy is defined as\\nH(X) = \\nX\\ni\\np(xi) log p(xi), (7)\\nwhere X is a random variable, xi are possible outcomes (here corresponding to acti-\\nvation values in feature maps), and p(xi) is the empirical probability of observing\\nxi. According to the proof in [29], this entropy change can be approximated by the\\nfollowing proxy formulation:\\nHl = (h  p + 1)(w  q + 1)  log\\n\\x0c\\x0cc11\\n\\x0c\\x0c, (8)\\nwhere h and w denote the height and width of the input feature map,p and q represent\\nthe height and width of the convolutional kernel, and c11 refers to the top-left coef-\\nficient of the convolution filter. This proxy captures how convolutional kernels affect\\nthe representational diversity of feature maps. A positive Hl implies increased repre-\\nsentational diversity, while a negative Hl indicates information compression. In this\\nwork, we extend the formulation to operate at the encoderblock level instead of indi-\\nvidual convolutional layers. Given an encoder block Bk containing m convolutional\\nlayers, we compute the average entropy change as:\\nH(Bk)\\navg = 1\\nm\\nX\\nlBk\\n\\x02\\nH(Fl+1)  H(Fl)\\n\\x03\\n. (9)\\nThis block-wise aggregation smooths local fluctuations and enables a more stable\\ncomparison across architectures and depths. We now provide a deeper analysis of the\\nentropy change patterns observed in Figure 4.2.\\nThe first encoder layer exhibits the largest entropy discrepancy between AE and VRAE\\nmodels. Specifically, AE3 shows a sharp entropy drop at Encoder 1, implying signifi-\\ncant information loss at the start, whereas VRAE models begin with higher entropy\\nchange, indicating richer information preservation and supporting the hypothesis that\\nmaintaining early entropy prevents premature collapse.\\nAs the encoding proceeds, AE models often display stronger and more irregular\\nentropy fluctuations across layers, while VRAEs maintain a smoother and more\\nconsistent decline. For example, VRAE3 decreases gradually from 1.5 to 3.7 (approx-\\nimately 2.2 units), compared to AE3s sharper drop from 6.3 to 3.5 (about 2.8 units).\\nThis smoother progression suggests that VRAEs handle information compression more\\neffectively, mitigating the risk of losing key features too early.\\nIn deeper layers (Encoders 45), both architectures tend to converge to similar entropy\\nlevels; however, AEs usually stabilize at lower entropy, reflecting more aggressive\\ncompression. VRAEs, in contrast, retain slightly higher entropy, indicating better\\npreservation of representational richness. Overall, these results reinforce that sustain-\\ning entropy diversity in early stages contributes to stronger representation learning in\\nVRAE.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='Fig. 3 Visualization of model performance trade-offs using the Pareto front. Each point denotes a\\nmodel evaluated by reconstruction quality (NMSE, PSNR, SSIM) versus inference speed (FPS), with\\ncolor indicating model size, i.e., the number of parameters. The red line represents the Pareto front,\\ni.e., the set of models that cannot be improved in one objective without sacrificing another. This\\nhighlights the optimal balance between accuracy and efficiency, helping to identify the most suitable\\nmodels depending on whether speed, quality, or a balanced trade-off is prioritized.\\n4.3 Comparative analysis of model trade-offs\\nTo evaluate the efficiency of different models in vehicle image restoration, we compare\\ntheir performance in terms of quality metrics (PSNR, SSIM, and NMSE) against\\ninference speed (FPS). Figure 3 shows the distribution of models with the Pareto\\nfront highlighted in red, while the color scale represents the number of parameters in\\nmillions.\\nIn the PSNRFPS and SSIMFPS plots, we observe that VRAE-based models\\n(e.g., VRAE3 and VRAE2) consistently lie on the Pareto front, striking a balance\\nbetween image quality and computational speed. In contrast, shallow autoencoders\\n(e.g., AE2 and AE3) achieve high FPS but suffer from significantly lower quality\\nscores, indicating that prioritizing speed alone comes at the cost of detail preservation.\\nMeanwhile, GAN and flow-based (FB) models demonstrate moderate performance,\\nbut their relatively high parameter counts make them less efficient.\\nThe NMSEFPS trade-off further highlights these limitations: although AE2\\nreaches the highest FPS, it exhibits considerably larger reconstruction error, while\\ndeeper VRAE variants (VRAE3, VRAE2) maintain low NMSE values with acceptable\\ninference speeds. These results underline the importance of designing models that pre-\\nserve sufficient depth of encoding to capture meaningful representations while avoiding\\nexcessive parameter growth.\\nOverall, the Pareto front demonstrates that VRAE architectures achieve a favor-\\nable trade-off across accuracy, speed, and parameter efficiency, making them strong\\ncandidates for real-time vehicle image restoration on edge devices.\\n4.4 Limitations\\nWhile the proposed VRAE models demonstrate superior reconstruction quality across\\nPSNR, NMSE, and SSIM, several limitations remain. As shown in Table 1, deeper\\nvariants (e.g., VRAE4 and VRAE5) yield consistent gains but incur substantially\\nhigher parameter counts and slower inference, revealing a clear trade-off between accu-\\nracy and efficiency. In addition, this work employs only base autoencoder and U-Net\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='architectures for clarity and accessibility, without exploring newer or more specialized\\ndesigns that could further enhance performance. Finally, the evaluation is conducted\\non a single dataset, which may limit the generalizability of the findings. Future work\\nwill address these aspects by extending to more efficient architectures and diverse\\ndatasets.\\n5 Conclusion and future work\\nVRAE was proposed the Variational Residual Autoencoder (VRAE) for vehicle image\\nrestoration, which combines an auxiliary encoder and feature aggregation to enhance\\nlocal detail and hierarchical representation. VRAE outperforms conventional Autoen-\\ncoders, GANs, and flow-based models in PSNR, SSIM, and NMSE, while offering\\ncompetitive inference speed. However, deeper VRAE variants improve quality at the\\ncost of higher complexity, limiting edge deployment. Future work includes explor-\\ning lightweight designs, compression methods, e.g., pruning, quantization, distillation,\\nand adaptive-depth architectures. VRAE can be further extended to real-time video\\nrestoration and other intelligent transportation tasks in future work.\\nReferences\\n[1] Pan, J., Sun, D., Pfister, H., Yang, M.-H.: Blind image deblurring using dark channel prior. In: Proceed-\\nings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16281636.\\nIEEE (2016)\\n[2] Kupyn, O., Budzan, V., Mykhailych, M., Mishkin, D., Matas, J.: Deblurgan: Blind motion deblurring\\nusing conditional adversarial networks. In: Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) Workshops, pp. 81838192. IEEE (2018)\\n[3] Nah, S., Kim, T.H., Lee, K.M.: Deep multi-scale convolutional neural network for dynamic scene\\ndeblurring. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 257265. IEEE (2017)\\n[4] Chen, C., Chen, Q., Xu, J., Koltun, V.: Learning to see in the dark. In: Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR) (2018). CVPR 2018\\n[5] Jiang, Y., Gong, X., Liu, D., Cheng, Y., Fang, C., Shen, X., Yang, J., Zhou, P.: Enlightengan: Deep\\nlight enhancement without paired supervision. arXiv preprint arXiv:1906.06972 (2019). arXiv 2019\\n[6] Zamir, S.W., Arora, A., Long, C., Sohail, A., Khan, S., Bhulla, M., Khan, F.S., Yang, M.-H., Khan,\\nM.A.: Learning enriched features for real image restoration and enhancement. In: European Conference\\non Computer Vision (ECCV) (2020). ECCV 2020\\n[7] Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of\\ndeep cnn for image denoising. IEEE Transactions on Image Processing 26(7), 31423155 (2017)\\n[8] Anwar, S., Barnes, N.: Real image denoising with feature attention. In: Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV) Workshops (2020). ICCV Workshops 2020\\n[9] Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767\\n(2018). arXiv 2018\\n[10] He, K., Gkioxari, G., Doll ar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE International\\nConference on Computer Vision (ICCV) (2017). ICCV 2017\\n[11] Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A.: Extracting and composing robust features\\nwith denoising autoencoders, 10961103 (2008). ACM\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[12] Kupyn, O., Martyniuk, T., Wu, J., Wang, Z.: Deblurgan-v2: Deblurring (orders-of-magnitude) faster\\nand better. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\\n88788887 (2019)\\n[13] Kingma, D.P., Dhariwal, P.: Glow: Generative flow with invertible 1x1 convolutions. In: Advances in\\nNeural Information Processing Systems, pp. 1021510224 (2018)\\n[14] Martin, S., Gagneux, A., Hagemann, P., Steidl, G.: Pnp-flow: Plug-and-play image restoration with\\nflow matching. International Conference on Learning Representations (ICLR) (2025)\\n[15] Zhu, Y., Zhao, W., Li, A., Tang, Y., Zhou, J., Lu, J.: Flowie: Efficient image enhancement via rectified\\nflow. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 1322 (2024)\\n[16] Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A.: Extracting and composing robust fea-\\ntures with denoising autoencoders. In: Proceedings of the 25th International Conference on Machine\\nLearning, pp. 10961103 (2008). ACM\\n[17] Xu, J., Li, Z., Guo, H., He, H.: Lightweight image super-resolution with information multi-distillation\\nnetwork. In: Proceedings of the 28th ACM International Conference on Multimedia, pp. 12371245\\n(2020)\\n[18] Jiang, K., Wang, Z., Yi, P., Wang, G., Lu, T., Jiang, J.: Deep distillation recursive network for remote\\nsensing image super-resolution. Remote Sensing 10(11), 1700 (2018)\\n[19] Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and new perspectives. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence 35(8), 17981828 (2013)\\n[20] Zhang, K., Zuo, W., Zhang, L.: Learning a single convolutional super-resolution network for multiple\\ndegradations. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 32623271 (2018)\\n[21] Ledig, C., Theis, L., Husz ar, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A., Tejani, A.,\\nTotz, J., Wang, Z., Shi, W.: Photo-realistic single image super-resolution using a generative adversarial\\nnetwork. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\n46814690 (2017)\\n[22] Wang, X., Yu, K., Dong, C., Loy, C.C.: Esrgan: Enhanced super-resolution generative adversarial\\nnetworks. Proceedings of the ECCV Workshops (2018)\\n[23] Zhang, K., Van Gool, L., Timofte, R.: Invertible image restoration. In: Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pp. 1008510094 (2021)\\n[24] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778 (2016)\\n[25] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks.\\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 47004708\\n(2017)\\n[26] Xu, Z., Yang, W., Meng, L., Lu, W., Huang, J., Zhou, J.: Towards end-to-end license plate detection\\nand recognition: A large dataset and baseline. European Conference on Computer Vision (ECCV)\\nWorkshops, 255271 (2018)\\n[27] Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversar-\\nial networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 11251134 (2017)\\n[28] Dinh, L., Sohl-Dickstein, J., Bengio, S.: Density estimation using real nvp. In: ICLR (2017)\\n[29] Meni, M.J., White, R.T., Mayo, M.L., Pilkiewicz, K.R.: Entropy-based guidance of deep neural net-\\nworks for accelerated convergence and improved performance. Information Sciences681, 121239 (2024)\\nhttps://doi.org/10.1016/j.ins.2024.121239\\n11')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919664e",
   "metadata": {},
   "source": [
    "# HuggingFace embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c8eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NTCUONG\\AppData\\Local\\Temp\\ipykernel_8372\\670216995.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Using HugginFace Sentence Transformer for embeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004014890640974045, 0.03706647455692291, -0.007565107196569443, -0.013695766218006611, 0.03984404355287552, 0.01877807453274727, -0.021859513595700264, 0.05953553691506386, -0.043915629386901855, 0.012312304228544235, -0.008210867643356323, 0.01599467545747757, -0.05861136317253113, 0.01461219321936369, 0.0048084743320941925, 0.04791124910116196, 0.0655793622136116, -0.0069184754975140095, 0.021358393132686615, -0.009512042626738548, -0.04826562479138374, 0.04322194680571556, 0.030967727303504944, 0.040927037596702576, -0.019166607409715652, 0.05017685890197754, 0.009344938211143017, 0.011121198534965515, -0.04760322347283363, 0.016301734372973442, 0.06417766958475113, -0.019559629261493683, 0.01678597182035446, -0.004271881189197302, -0.013319393619894981, 0.013867324218153954, -0.043819360435009, 0.005989827215671539, -0.006994844879955053, -0.00043819003622047603, -0.003710720920935273, 0.01657402142882347, 0.02120954357087612, 0.03943435102701187, -0.03594643995165825, -0.007666497025638819, -0.11285924166440964, 0.017585618421435356, -0.015436507761478424, -0.052247531712055206, -0.12399986386299133, -0.04666425660252571, -0.021805420517921448, -0.009821905754506588, 0.01975947991013527, 0.027178345248103142, 0.014273273758590221, -0.1344124972820282, -0.00766670610755682, -0.008646267466247082, 0.0051152147352695465, 0.008878718130290508, 0.04289424419403076, -0.011274286545813084, 0.0021416260860860348, -0.042455028742551804, -0.05066238343715668, -0.019115805625915527, -0.08498368412256241, -0.10583048313856125, 0.016557609662413597, 0.06392746418714523, -0.05319518223404884, -0.06330293416976929, 0.008809917606413364, -0.04857728257775307, -0.026957962661981583, 0.037246283143758774, 0.05682910606265068, 0.01983775943517685, 0.026713984087109566, 0.06331156194210052, -0.001549038104712963, 0.045831289142370224, -0.006304641719907522, 0.0077210343442857265, 0.0028401892632246017, -0.01952364481985569, -0.018140461295843124, 0.05161247029900551, 0.01691613346338272, -0.02571076899766922, 0.01924014836549759, 0.04532492905855179, 0.007414256688207388, 0.01697704941034317, 0.042018383741378784, -0.0030773282051086426, -0.005916141904890537, 0.05731083080172539, -0.0033345690462738276, -0.05154900625348091, 0.010895054787397385, -0.005906836595386267, -0.06830116361379623, -0.03269462287425995, -0.008984087035059929, 0.006925247143954039, 0.010053213685750961, -0.07120077311992645, 0.017769064754247665, -0.019193777814507484, -0.001819611294195056, 0.02603127248585224, -0.009047129191458225, 0.025187594816088676, -0.029480913653969765, 0.0031330378260463476, -0.014655212871730328, -0.03283502161502838, 0.03502878174185753, 0.03942236304283142, -0.029335668310523033, 0.05937279760837555, -0.006334173027426004, 0.002104378305375576, -0.015409286133944988, 0.04877801612019539, -0.042739372700452805, -0.04198457673192024, 0.013296922668814659, 0.04072990268468857, -0.021170662716031075, -0.029037408530712128, -0.023436423391103745, 0.03847155719995499, 0.0039289784617722034, 0.03305516019463539, 0.017137086018919945, -0.014449704438447952, 0.005114942789077759, 0.009386375546455383, 0.019422879442572594, -0.032999373972415924, 0.03771253302693367, 0.051094900816679, -0.027090327814221382, -0.029823394492268562, 0.011931153014302254, 0.009202901273965836, -0.042868472635746, 0.02213008515536785, 0.007702985778450966, 0.008190354332327843, 0.004517014138400555, 0.0235213004052639, 0.004399603232741356, -0.00436601135879755, -0.018161095678806305, -0.0035907786805182695, -0.02129530720412731, -0.010629141703248024, -0.08682478964328766, -0.01595890335738659, -0.01067465078085661, -0.005792103707790375, -0.0008068092865869403, -0.0010752988746389747, 0.009100992232561111, -0.010765637271106243, -0.025761866942048073, -0.03225633502006531, 0.05996021628379822, -0.048634372651576996, -0.02568727545440197, -0.02905282936990261, 0.04186037555336952, 0.01268115546554327, 0.02553921937942505, 0.031609442085027695, -0.07175648957490921, 0.0684351697564125, 0.01332691591233015, -0.01598837412893772, 0.01290093082934618, -0.003015022724866867, 0.11775114387273788, -0.04463233798742294, -0.029949909076094627, 0.0007136741187423468, -0.07095830887556076, -0.05281347408890724, 0.023280270397663116, 0.011861958540976048, 0.016138747334480286, -0.03737581893801689, -0.011500812135636806, 0.11456108093261719, 0.0240440983325243, 0.031835559755563736, 0.04971057549118996, 0.02698853239417076, -0.008172476664185524, 0.0038898992352187634, 0.0047371359542012215, -0.0125660952180624, 0.0580594539642334, -0.0150078060105443, -0.06371919065713882, 0.02601747214794159, 0.007034032605588436, -0.013666609302163124, 0.05656258389353752, 0.0004971218877471983, 0.02810250222682953, -0.0036719050258398056, -0.045213546603918076, -0.017638057470321655, -0.032371729612350464, -0.031183624640107155, 0.043064117431640625, -0.05132368206977844, -0.03198445960879326, 0.012524076737463474, -0.02747560851275921, 0.044322721660137177, 0.020510517060756683, 0.0008206030470319092, 0.009934947825968266, 0.015271819196641445, 0.00406662467867136, -0.038004521280527115, 0.012316628359258175, -0.07467364519834518, 0.025931639596819878, -0.029121622443199158, -0.024856850504875183, -0.026238054037094116, 0.014896978624165058, -0.0040779816918075085, 0.01059738826006651, 0.05283394828438759, -0.05080411210656166, 0.016396623104810715, 0.018432263284921646, 0.029320478439331055, -0.0007593396585434675, -0.03287995979189873, -0.013624414801597595, -0.051683299243450165, 0.001264240243472159, -0.0021197230089455843, -0.022071130573749542, -0.0017233641119673848, -0.021515898406505585, -0.07109060883522034, -0.006675441283732653, -0.017492691054940224, -0.003984369337558746, 0.059574831277132034, 0.026521315798163414, -0.017997434362769127, -0.007161364424973726, -0.0008086767047643661, -0.002456173300743103, -0.07384653389453888, 0.00582874147221446, -0.046902816742658615, 0.026820797473192215, 0.011700487695634365, 0.0642135888338089, 0.0041433279402554035, 0.009570438414812088, -0.03370959311723709, 0.013586423359811306, 0.01635945588350296, 0.00705753406509757, 0.00911769736558199, -0.11459432542324066, -0.045968759804964066, -0.040029674768447876, 0.03772110491991043, -0.023656638339161873, -0.0381162203848362, -0.022772088646888733, -0.024948107078671455, 0.0062459358014166355, -0.06467276811599731, 0.03323917090892792, 0.03335610777139664, 0.03549737110733986, 0.0204653088003397, -0.0382576510310173, 0.0006380418199114501, 0.032610923051834106, 0.04358874261379242, 0.01638757809996605, 0.02672288566827774, 0.013868195004761219, 0.061970487236976624, -0.03062995709478855, 0.009264038875699043, -0.009702620096504688, 0.06774569302797318, 0.010848785750567913, -0.011858194135129452, 0.02093958854675293, -0.007055737543851137, -0.2811815142631531, -0.017789164558053017, -0.021057188510894775, -0.05337924510240555, -0.01067697536200285, -0.02301258221268654, 0.02120603621006012, -0.04119202494621277, -0.006513272877782583, -0.02602488175034523, -0.0549076683819294, 0.0010496254544705153, -0.03328487649559975, 0.0023144881706684828, 0.039170194417238235, 0.039363496005535126, -0.04532169550657272, -0.027897385880351067, 0.013803844340145588, 0.03738555312156677, -0.026572631672024727, -0.021556654945015907, 0.0011824793182313442, 0.018323972821235657, -0.007561299949884415, 0.015002485364675522, -0.06325713545084, 0.044342245906591415, -0.006401988212019205, 0.002768510254099965, -0.02882547676563263, -0.018576590344309807, 4.08733612857759e-05, -0.0038079123478382826, -0.01812639646232128, 0.010192752815783024, 0.04973214492201805, 0.016707882285118103, 0.049968305975198746, -0.03316257894039154, 0.03524722903966904, -0.024631701409816742, -0.010371668264269829, -0.02507873997092247, 0.06004336103796959, -0.03140432760119438, -0.0612606406211853, -0.03639961779117584, -0.018411515280604362, 0.07105287909507751, -0.04781658202409744, -0.027534453198313713, -0.009453989565372467, 0.022147316485643387, 0.0291771050542593, -0.008781012147665024, 0.0007864410290494561, -0.021490052342414856, -0.03247326239943504, 0.060029271990060806, 0.0032557561062276363, -0.04483562335371971, -0.01988474652171135, -0.0577516183257103, -0.03430012986063957, -0.031883906573057175, -0.025612005963921547, -0.03407015651464462, 0.022313715890049934, 0.052026696503162384, -0.060184359550476074, 0.05439188703894615, 0.00620220135897398, -0.0789690613746643, -0.0165045578032732, -0.02926160953938961, 0.008438948541879654, -0.0431167408823967, 0.005859054625034332, -0.010138026438653469, -0.015970155596733093, -0.03886598348617554, -0.0014763035578653216, 0.021991729736328125, 0.012246217578649521, -0.030573489144444466, 0.034104906022548676, 0.014774683862924576, -0.02419346384704113, -0.016047636047005653, -0.004133219365030527, 0.0039871493354439735, 0.013985314406454563, 0.036000240594148636, 0.04566660895943642, -0.004948860500007868, 0.026675237342715263, 0.03761325404047966, -0.001589988823980093, 0.007692711893469095, 0.03810715302824974, -0.04488345980644226, -0.01818283274769783, -0.014692675322294235, -0.013010760769248009, 0.011186141520738602, -0.051319509744644165, -0.005061828065663576, 0.0669262632727623, 0.03353690728545189, -0.012724034488201141, 0.04256308451294899, 0.0411984957754612, -0.018828729167580605, -0.06227785348892212, -0.033218108117580414, -0.008860860019922256, 0.010993877425789833, 0.0028278420213609934, -0.025283461436629295, -0.038626253604888916, 0.03403768688440323, 0.023579232394695282, 0.019618015736341476, -0.01963133178651333, 0.001135542755946517, -0.011542098596692085, 0.001256468240171671, -0.034429434686899185, -0.035543590784072876, -0.020984884351491928, -0.003807295812293887, -0.008743161335587502, -0.020874546840786934, 0.019030818715691566, 0.018464038148522377, -0.024156859144568443, -0.026957480236887932, 0.02927539311349392, 0.005985392723232508, 0.018820373341441154, 0.030768806114792824, 0.03861934319138527, -0.03162116929888725, -0.009574299678206444, -0.009731491096317768, -0.00119208381511271, 0.07290683686733246, 0.01091072615236044, -0.003456170903518796, -0.0016972075682133436, -0.01716570556163788, 0.013230939395725727, -0.01054589543491602, 0.015212701633572578, -0.015073263086378574, 0.0554971881210804, 0.0009975024731829762, 0.05464816465973854, -0.07192699611186981, 0.053191035985946655, 0.009669321589171886, 0.005127259995788336, 0.0031680818647146225, 0.02157817780971527, 0.042335860431194305, -0.012530183419585228, 0.020759155973792076, -0.0009049635264091194, 0.02001136541366577, 0.026366345584392548, -0.008361493237316608, -0.019393345341086388, -0.0007574636256322265, -0.016285544261336327, 0.0089272977784276, 0.000811658101156354, -0.015003073029220104, -0.021469099447131157, 0.08362942188978195, 0.016693104058504105, 0.007971307262778282, -0.007324111647903919, 0.05827978998422623, 0.04025415703654289, 0.03914756327867508, 0.02590174600481987, 0.04528963938355446, -0.04357055947184563, 0.011408326216042042, 0.01875457540154457, 0.04501708596944809, -0.014387334696948528, -0.009520843625068665, -0.06763073056936264, -0.05955326929688454, -0.006161597557365894, 0.004431499168276787, 0.013352024368941784, -0.006745541002601385, 0.012156892567873001, -0.02727777324616909, -0.04567825794219971, 0.004280407913029194, 0.04728514328598976, -0.0453987643122673, 0.004120308440178633, 9.05044871615246e-05, 0.06756103038787842, 0.02479173056781292, -0.01991240121424198, -0.05391392856836319, -0.09193723648786545, -0.02797655202448368, 0.00020878865325357765, 0.008209891617298126, -0.019987493753433228, -0.009012021124362946, -0.010316706262528896, -0.0022448841482400894, 0.001182036241516471, -0.010544802993535995, -0.032335471361875534, -0.005385264288634062, -0.013082862831652164, 0.01489926502108574, -0.03318246454000473, 0.00922379456460476, 0.004304915200918913, -0.0030306812841445208, 0.0310102179646492, -0.047579940408468246, 0.032774750143289566, 0.02867160178720951, 0.03500473126769066, 0.028681209310889244, -0.043915923684835434, 0.03949206322431564, 0.03613406792283058, 0.01792442612349987, 0.004243400413542986, 0.03247091919183731, 0.010845924727618694, -0.020656177774071693, -0.011404837481677532, 0.057228077203035355, 0.0017154059605672956, 0.020191919058561325, -0.016044801101088524, 0.00392988882958889, -0.023042021319270134, 0.052486877888441086, -0.01946740224957466, 0.016774749383330345, 0.023863481357693672, -0.01841915398836136, -0.061116546392440796, -0.07055259495973587, -0.05483909323811531, -0.03705421835184097, -0.00977668259292841, 0.011235697194933891, -0.0034113076981157064, 0.009661482647061348, 0.052043616771698, 0.07632313668727875, -0.013769928365945816, -0.032410990446805954, 0.06765353679656982, 0.01576845720410347, -0.016441205516457558, -0.057798076421022415, -0.003525764914229512, 0.062465887516736984, -0.026921456679701805, -0.05253344774246216, -0.009617525152862072, -0.027906134724617004, -0.042759671807289124, -0.00172452162951231, -0.0482240691781044, 0.011379427276551723, -0.04170629754662514, 0.021739544346928596, 0.013491188175976276, -0.025791486725211143, 0.011117568239569664, -0.007252573035657406, -0.02510952763259411, -0.024099035188555717, -0.013122246600687504, 0.030534978955984116, -0.024298163130879402, 0.014695016667246819, -0.025795554742217064, -0.004295590333640575, 0.039627403020858765, 0.05842536687850952, -0.013332015834748745, 0.028621725738048553, 0.007139361463487148, 0.03570308908820152, 0.057618871331214905, 0.023639924824237823, 0.004841913003474474, 0.006844281684607267, -0.03761318698525429, -0.06346961855888367, 0.04273901879787445, 0.037080954760313034, -0.004991395864635706, -0.010601311922073364, 0.04689450189471245, 0.06180976703763008, -0.056500937789678574, -0.005537028424441814, 0.029623212292790413, -0.03740837797522545, -0.009223802015185356, 0.006540452595800161, -0.023088598623871803, -0.012401469983160496, 0.028850136324763298, 0.028053799644112587, 0.043979112058877945, 0.07027989625930786, -0.015249972231686115, -0.008095761761069298, 0.013859023340046406, 0.07627096772193909, 0.009880142286419868, 0.07306472957134247, 0.04270648583769798, 0.02988567017018795, -0.0509444922208786, -0.056236155331134796, -0.038664110004901886, 0.06022133305668831, 0.012986190617084503, -0.018540309742093086, 0.013386214151978493, 0.024999836459755898, -0.010889642871916294, 0.05794524773955345, 0.0030707826372236013, 0.011812308803200722, 0.023215875029563904, 0.05191691964864731, -0.006895675789564848, 0.042339105159044266, 0.04174748435616493, 0.03883175551891327, 0.003219691338017583, 0.02157740108668804, 0.03443554416298866, -0.06900868564844131, -0.03607191890478134, 0.06564714014530182, 0.002380338264629245, 0.004436092916876078, -0.014613430947065353, 0.07609415799379349, 0.11974380910396576, -0.05797161906957626, -0.0060640121810138226, -0.007014218717813492, 0.04047486558556557, -0.007176812272518873, 0.03498174622654915, -0.0016975010512396693, -0.011202797293663025, -0.02250397950410843, -0.02958628721535206, -0.0314859077334404, 0.01998111791908741, 0.006025992333889008, 0.040435485541820526, -0.007043611723929644, -0.05979396775364876, 0.010950843803584576, 0.007127419579774141, -0.037801146507263184, -0.03563760221004486, -0.016387496143579483, -0.03490430489182472, -0.02110021375119686, -0.013343406841158867, 0.035841118544340134, -0.012287289835512638, -0.018568435683846474, 0.02126459777355194, 0.005315098911523819, -0.014567806385457516, 0.04953409731388092, -0.019313545897603035, -0.011840886436402798, 0.018850592896342278, 0.019030220806598663, -0.006455337628722191, 0.03439437970519066, 0.012702979147434235, -0.042435675859451294, -0.009003529325127602, -0.001257208758033812, 0.033717747777700424, 0.001818371587432921, 0.010148520581424236, 0.01824902929365635, -0.07748492807149887, -0.009739004075527191, 0.052918460220098495, -0.018290044739842415, -0.05797126144170761, 0.028196142986416817, 0.029848629608750343, 0.05060514062643051, 0.02595275267958641, 0.043270740658044815, -0.004579978995025158, 0.023381438106298447, -0.04025222361087799, 0.035198550671339035, 0.04006052017211914, 0.06555265933275223, -0.0060798353515565395, 0.07142510265111923, 0.01749386079609394, 0.009828716516494751, -0.05738608166575432, 0.012865494936704636, -0.017041608691215515, -0.06739313155412674, -0.009243006817996502, -0.003235764568671584, -0.013772759586572647, -0.025417765602469444, 0.023830998688936234, -0.022874297574162483, -0.05409885570406914, -0.05275903642177582, 0.010763941332697868, -0.03830138221383095, 0.012155178934335709, -0.02121782675385475, 0.002543495735153556, -0.04415806010365486, -0.02044103294610977, 0.027658948674798012, -0.05294301360845566, -0.02788933366537094, 0.001958746463060379, -0.014078079722821712, 0.014350340701639652, -0.05067608132958412, 0.010584595613181591, 0.06768932193517685, 0.0526442788541317, -0.07156515121459961, 0.01604822650551796, 0.0015894388779997826]\n"
     ]
    }
   ],
   "source": [
    "# Example text to embed\n",
    "text = \"LangChain is a framework for developing applications powered by language models.\"\n",
    "vector = embedding_model.embed_query(text)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5e9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='VRAE: Vertical Residual Autoencoder for License\\nPlate Denoising and Deblurring\\nCuong Nguyen1, Dung T. Tran1,2, Hong Nguyen3,\\nXuan-Vu Phan1, Nam-Phong Nguyen1*\\n1Ha Noi University of Science and Technology, Ha Noi, Vietnam.\\n2Center of Environmental Intelligence, VinUniversity, Hanoi, Vietnam.\\n3University of Southern California, Los Angeles, United States.\\n*Corresponding author(s). E-mail(s): phong.nguyennam@hust.edu.vn;\\nContributing authors: cuong.nt227090@sis.hust.edu.vn;\\ndung.tt2@vinuni.edu.vn; hongn@usc.edu; vu.phanxuan@hust.edu.vn;\\nAbstract\\nIn real-world traffic surveillance, vehicle images captured under adverse weather,\\npoor lighting, or high-speed motion often suffer from severe noise and blur. Such\\ndegradations significantly reduce the accuracy of license plate recognition sys-\\ntems, especially when the plate occupies only a small region within the full\\nvehicle image. Restoring these degraded images a fast realtime manner is thus\\na crucial pre-processing step to enhance recognition performance. In this work,\\nwe propose a Vertical Residual Autoencoder (VRAE) architecture designed for\\nthe image enhancement task in traffic surveillance. The method incorporates an\\nenhancement strategy that employs an auxiliary block, which injects input-aware\\nfeatures at each encoding stage to guide the representation learning process,\\nenabling better general information preservation throughout the network com-\\npared to conventional autoencoders. Experiments on a vehicle image dataset\\nwith visible license plates demonstrate that our method consistently outperforms\\nAutoencoder (AE), Generative Adversarial Network (GAN), and Flow-Based\\n(FB) approaches. Compared with AE at the same depth, it improves PSNR by\\nabout 20%, reduces NMSE by around 50%, and enhances SSIM by 1%, while\\nrequiring only a marginal increase of roughly 1% in parameters.\\nKeywords: License Plate Deblurring, License Plate Denoising, Lightweight,\\nGeneralize Preservation\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='1 Introdution & Related Work\\nIn recent years, traffic scene understanding has become essential for intelligent trans-\\nportation, autonomous driving, and surveillance. However, real-world traffic images\\nare often degraded, reducing system reliability. Motion blur frequently occurs due to\\nfast vehiclecamera motion and low-cost sensors with limited exposure, while advanced\\nhigh-speed cameras remain costly [13]. Moreover, low-light conditions further intro-\\nduce noise, glare, and low contrast, particularly in Southeast Asian cities with dense\\nnighttime traffic and mixed lighting [46]. In addition, sensor limitations such as noise\\nand dynamic range exacerbate the trade-off between blur and signal strength [7, 8].\\nAs a result, these degradations severely impair downstream tasks like detection, recog-\\nnition, and segmentation [9, 10]. To address this, image enhancement techniques,\\nespecially including denoiser and deblurrer, are widely studied. While traditionally\\ntreated separately, traffic images often suffer from both simultaneously. Deep learn-\\ning methods have been explored including AE for compact representations, GAN for\\nenhanced perceptual quality, and FB for explicit distribution learning that balances\\nfidelity and diversity.\\nTraditional methods using AE have been widely adopted in such tasks due to their\\nability to learn compact latent representations and reconstruct images by filtering noise\\nfrom signal [11]. Classical convolutional AE have shown promising results in removing\\nadditive noise, as demonstrated in DnCNN [7], which employed residual learning and\\nbatch normalization for Gaussian denoising. Similarly, convolutional encoder-decoder\\nstructures have been successfully applied in deblurring, such as DeblurGAN [2], which\\ncombined adversarial training with perceptual loss to recover sharp textures. This facil-\\nitates direct mapping from corrupted inputs to clean outputs, effectively addressing\\ncomplex or spatially varying degradations. State-of-the-art deep generative methods,\\nsuch as GAN and Diffusion, have been integrated into restoration pipelines. Specifi-\\ncally, DeblurGAN-v2 [12] introduced a multi-scale GAN framework that achieved both\\nsharpness and stability in deblurring. Recently, FB models have emerged as a princi-\\npled alternative for image restoration, capable of learning invertible mappings between\\ndegraded and clean domains while preserving density information [13].Notably, FB\\nhas been introduced as a scalable and stable training framework for conditional image\\ngeneration, with its application to restoration explored in tasks demanding spatial\\nconsistency and smooth transitions, such as PnP-Flow [14, 15].\\nResearch Gap: AE have been employed in traffic image enhancement due to\\ntheir lightweight design, which is suitable for resource-constrained monitoring sys-\\ntems [1618]. However, shallow encoders typically capture only low-level features and\\nfail to recover mid- and high-level structures that are critical for fine detail restora-\\ntion [19, 20]. GAN improve perceptual realism with sharper edges and more natural\\ntextures, but often distort pixel-level fidelity, hallucinate numbers, limiting their reli-\\nability for traffic applications where accurate recovery of details such as license plates\\nand road markings is essential [21, 22]. FB models preserve pixel-level accuracy by\\noptimizing explicit likelihood functions, yet their high computational complexity and\\nmemory consumption make them impractical for real-time deployment on edge devices\\n[13, 23]. Therefore, a clear research gap exists in developing architectures that balance\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content=\"computational efficiency with accurate detail preservation for traffic image enhance-\\nment. For such reasons, the main contributions of this work can be summarized as\\nfollows:\\n We propose the Vertical Residual Autoencoder (VRAE)with an auxiliary\\nblock that vertically aggregates and refines embeddings for enhanced feature repre-\\nsentation. Experimental results show that VRAE achieves impressive improvements\\nacross PSNR, SSIM, and NMSE, while maintaining competitive performance even\\nwith reduced model parameters.\\n Through extensive experiments, we show that our approach not only encourages\\ngeneral information preservation in the early encoder layers but also outperforms\\nconventional autoencoders in terms of image compression, as evaluated by entropy\\nchange\\nx x  \\nResnet50 stage \\nMain block Auxiliary block\\nDecoder\\nX 1 x 3 x 4 x 5 X 2 \\nX' 1 X' 2 x' 3 x' 4 \\nE 1 E 2 E 3 E 4 E 5 \\nE 1 ' \\nE 2 ' E 3 ' E 4 ' \\nD \\nAuxiliary encoder Addition\\nNext\\nFig. 1 VRAE aggregates the input embedding by passing it through the feature embedding block\\nbefore forwarding it to the next encoder block.\\n2 Vertical Residual Autoencoder\\nProblem statementIn traffic surveillance systems, vehicle images are often degraded\\nby motion blur, sensor noise or adverse weather conditions, which obscure important\\nvisual details such as license plates, headlights, and road markings. Assume that all\\ndistortion is created by   N(0, 1) and the task of vehicle image enhancement is\\ndefined as learning a one-to-one mapping F: x  x from a degraded input image\\nx  R3WH to an enhanced output image x  R3WH, where x = x + .\\n2.1 Overall Architecture\\nVRAE comprises three components: (i) a parallel stack ofAuxiliary Encoders{Ei}N\\ni=1,\\neach extracting hierarchical features directly from the inputx; (ii) a continuous stack of\\nMain Encoders{E\\ni}N1\\ni=1 that further transforms the intermediate features and embeds\\nthem for residual injection into deeper stages; and (iii) a Decoder D that aggregates\\nfeatures from both streams to reconstruct the output x. The two streams interact via\\nelement-wise additions at multiple stages, enabling vertical residual learning.\\n3\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='2.2 Auxiliary Block\\nThe auxiliary path {E\\ni} acts as a feature embedding modulethat repeatedly extracts\\ncomplementary, input-conditioned features to be fused with the main stream. Each\\nblock follows a shallow ConvNormActivation structure and optionally includes\\npooling to summarize global context:\\nx\\ni = E\\ni(x), i = 1, . . . ,5. (1)\\nAs illustrated in Fig. 1, the auxiliary block operates in parallel with the main\\nencoder and generates an additional feature hierarchy {x\\ni} from the degraded input\\nx. These features are progressively injected into the corresponding layers of the main\\nblock through element-wise fusion, allowing the model to enrich low- and mid-level\\nrepresentations with complementary cues.\\n2.3 Main Block\\nIn our implementation, each main encoder Ei is instantiated as the i-th stage of the\\nResNet-50 architecture [24]. ResNet-50 employs a bottleneck design consisting of three\\nconvolutional layers: a 1  1 convolution for dimensionality reduction, a 3  3 convo-\\nlution for spatial processing, and another 1  1 convolution for restoring dimensions.\\nA skip connection adds the input to the output of the block, enabling better gradi-\\nent flow and alleviating the vanishing gradient problem. To ensure a fair comparison,\\nthe same ResNet-50based encoder backbone is also adopted for the baseline Autoen-\\ncoder (AE), GAN, and FB models. This guarantees that the observed performance\\nimprovement arises from the proposed vertical residual aggregation mechanism rather\\nthan architectural capacity differences.\\nAt each stage, the feature maps from the main and auxiliary encoder paths are\\nfused by element-wise addition before being passed into Ei. Formally:\\nxi = Ei\\n\\x00\\nxi1 + x\\ni1\\n\\x01\\n, i = 2, . . . ,5, (2)\\nwhere xi1 is the output of the (i1)-th main encoder block and x\\ni1 is the output of\\nthe (i1)-th auxiliary encoder block. This additive fusion serves as an embedding-level\\nintegration mechanism: it incorporates complementary information without increas-\\ning feature dimensionality, offering a parameter-efficient alternative to concatenation.\\nSuch design, inspired by residual networks [24] and multi-branch models [25], not only\\nstabilizes gradient flow but also enhances representational power, allowing the main\\nencoder to capture hierarchical abstractions while continuously integrating localized\\ncues from the auxiliary encoder.\\n2.4 Decoder\\nThe decoder D reconstructs x from xN using a cascade of transposed convolu-\\ntional layersfollowed by ReLU activations. Each transposed convolution progressively\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='upsamples the feature maps, restoring the spatial resolution:\\nyk = ReLU(ConvTranspose2dk(yk1)) , k = 1, . . . ,5, y0 = xN, yK = x. (3)\\nWe train the network with the mean squared error (MSE)loss only:\\nLMSE = 1\\nBCHW\\nBX\\nb=1\\nCX\\nc=1\\nHX\\ni=1\\nWX\\nj=1\\n\\x00\\nxb,c,i,j  xb,c,i,j\\n\\x012\\n, (4)\\nwhich averages the squared reconstruction error over the entire batch and all\\npixels/channels.\\n3 Experiments\\n3.1 Dataset Preparation\\nFrom the CCPD dataset [26], 3,036 high-resolution vehicle images were collected and\\nresized to 256256 pixels. The dataset was split into training, validation, and test sets\\nin a 70%15%15% ratio. Data augmentation, including rotations, was applied only to\\nthe training set, increasing it to 7,000 images. Low-resolution inputs were generated\\nby adding discrete noise (values in [0 , 9] scaled by 0.1) and applying a 3  3 average\\npooling filter iteratively 10 times, simulating low-quality surveillance footage. This\\ndegradation process is not identical but bears similarity to the synthetic degradations\\nused in SRMD [20], where combinations of blurring and noise are employed to simulate\\nmultiple low-resolution conditions.\\n3.2 Baseline choices\\nIn vehicle image restoration, GAN have been widely applied to inpainting, deblurring,\\nsuper-resolution, and artifact removal, achieving visually plausible and high-quality\\nresults [12, 21, 27]. In contrast, flow-based models such as RealNVP [28] and Glow [13]\\nprovide exact invertibility between image and latent spaces, enabling better preser-\\nvation of fine-grained details. Although still underexplored in this domain, these\\ncharacteristics make flow-based approaches particularly relevant to vehicle image\\nrestoration. To ensure a fair and controlled comparison, in this work we select both\\nGAN and FB methods as baselines and design them with main encoder and decoder\\nblocks consistent with our proposed method.\\n3.3 Settings\\nAll models were trained using the Adam optimizer with an initial learning rate of\\n1104 and a batch size of 16. Each model was trained for 100 epochs on an NVIDIA\\nRTX 4070 GPU.\\n AE, FB, VRAE: These methods utilize the MSE loss between the predicted and\\nground-truth HR images\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Table 1 Quantitative comparison of GAN, FB, AE (25 sub encoders), and the proposed\\nVRAE (25 sub-encoders). All metrics are evaluated on the test setand averaged across\\nall samples. The best results are in bold, and the second-best are underlined.\\nModel PSNR ( ) NMSE ( ) SSIM ( ) Params FPS\\nGAN (Gen) 28.743 0.006 0.842 14.59M 188\\nFB (Gen) 27.093 0.008 0.831 25.87M 35\\nAE2 27.787 0.007 0.840 0.375M1 411\\nAE3 26.159 0.011 0.802 2.77M 289\\nAE4 29.404 0.005 0.864 14.59M 189\\nAE5 27.243 0.008 0.793 48.43M 119\\nVRAE2 30.319 0.004 0.884 0.376M 399\\nVRAE3 31.052 0.003 0.898 2.78M 194\\nVRAE4 30.246 0.004 0.880 14.61M 90\\nVRAE5 30.032 0.003 0.860 48.48M 45\\n1M denotes millions, the unit of the number of parameters.\\n GAN: The GAN generator is trained with a hybrid loss combining pixel-wise MSE\\nand adversarial BCE. While MSE ensures closeness to the ground truth, it often\\nleads to overly smooth outputs. The adversarial loss encourages perceptual realism\\nby pushing the generator to fool the discriminator. The overall generator loss is thus:\\nLGEN = LMSE +   LBCE\\nwhere  = 10 3 is a small scalar balancing the adversarial loss contribution,\\nfollowing the original setting proposed in [21].\\n4 Result\\n4.1 Comparison with State-of-the-Art\\nTable 1 presents the quantitative comparison among GAN, FB, AE (25 stacks),\\nand the proposed VRAE (25 sub-encoders). Conventional GAN-based and FB-based\\napproaches show moderate perceptual quality but achieve relatively lower PSNR\\nand SSIM values compared to AE-based and VRAE-based models. Although stacked\\nAutoencoders (e.g., AE4) can provide competitive results (PSNR = 29.404 dB, SSIM\\n= 0.864), their performance is inconsistent across different stack depths, and some con-\\nfigurations (e.g., AE3 and AE5) degrade significantly. In contrast, the proposed VRAE\\nconsistently outperforms both AE and GAN variants in terms of reconstruction qual-\\nity. Notably, VRAE3 achieves the highest PSNR (31.052 dB) and SSIM (0.898), while\\nalso maintaining a low NMSE (0.003). VRAE2 also delivers strong results (PSNR =\\n30.319 dB, SSIM = 0.884), ranking second overall. Despite slightly higher parameter\\ncounts than AE2, VRAE models demonstrate a favorable trade-off between accuracy\\nand efficiency, with FPS values remaining competitive (e.g., VRAE2 at 399 FPS versus\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='Fig. 2 Average entropy change across encoder layers of AE and VRAE. In the proposed architecture,\\nentropy is computed from synthesized embeddings aggregated across encoder stages, capturing both\\nlocal and global contextual dependencies.\\nAE2 at 411 FPS). When compared with AE at the same stack depth, VRAE improves\\nPSNR by approximately 20%, reduces NMSE by around 50%, and enhances SSIM by\\nabout 1%, while introducing only a marginal increase of roughly 1% in the number\\nof parameters. These results indicate that the VRAE architecture not only stabilizes\\nperformance across different depths but also surpasses conventional baselines in both\\nreconstruction fidelity and computational efficiency.\\n4.2 Informative content preservation\\nAccording to [29], encouraging entropy preservation in the early layers promotes better\\ngeneralization. This is consistent with our hypothesis Figure 4.2, where we observe\\nthat models preserving higher entropy in the first encoder layers tend to achieve better\\nstability across subsequent layers. We first define the feature maps at encoder layerl as\\nFl  RClHlWl , (5)\\nwhere Cl is the number of channels, and Hl, Wl are the spatial height and width of\\nthe feature maps. Formally, the entropy change between consecutive layers l and l + 1\\nis defined as\\nHl = H(Fl+1)  H(Fl), (6)\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='where H(Fl) denotes the Shannon entropy of the feature maps Fl. The Shannon\\nentropy is defined as\\nH(X) = \\nX\\ni\\np(xi) log p(xi), (7)\\nwhere X is a random variable, xi are possible outcomes (here corresponding to acti-\\nvation values in feature maps), and p(xi) is the empirical probability of observing\\nxi. According to the proof in [29], this entropy change can be approximated by the\\nfollowing proxy formulation:\\nHl = (h  p + 1)(w  q + 1)  log\\n\\x0c\\x0cc11\\n\\x0c\\x0c, (8)\\nwhere h and w denote the height and width of the input feature map,p and q represent\\nthe height and width of the convolutional kernel, and c11 refers to the top-left coef-\\nficient of the convolution filter. This proxy captures how convolutional kernels affect\\nthe representational diversity of feature maps. A positive Hl implies increased repre-\\nsentational diversity, while a negative Hl indicates information compression. In this\\nwork, we extend the formulation to operate at the encoderblock level instead of indi-\\nvidual convolutional layers. Given an encoder block Bk containing m convolutional\\nlayers, we compute the average entropy change as:\\nH(Bk)\\navg = 1\\nm\\nX\\nlBk\\n\\x02\\nH(Fl+1)  H(Fl)\\n\\x03\\n. (9)\\nThis block-wise aggregation smooths local fluctuations and enables a more stable\\ncomparison across architectures and depths. We now provide a deeper analysis of the\\nentropy change patterns observed in Figure 4.2.\\nThe first encoder layer exhibits the largest entropy discrepancy between AE and VRAE\\nmodels. Specifically, AE3 shows a sharp entropy drop at Encoder 1, implying signifi-\\ncant information loss at the start, whereas VRAE models begin with higher entropy\\nchange, indicating richer information preservation and supporting the hypothesis that\\nmaintaining early entropy prevents premature collapse.\\nAs the encoding proceeds, AE models often display stronger and more irregular\\nentropy fluctuations across layers, while VRAEs maintain a smoother and more\\nconsistent decline. For example, VRAE3 decreases gradually from 1.5 to 3.7 (approx-\\nimately 2.2 units), compared to AE3s sharper drop from 6.3 to 3.5 (about 2.8 units).\\nThis smoother progression suggests that VRAEs handle information compression more\\neffectively, mitigating the risk of losing key features too early.\\nIn deeper layers (Encoders 45), both architectures tend to converge to similar entropy\\nlevels; however, AEs usually stabilize at lower entropy, reflecting more aggressive\\ncompression. VRAEs, in contrast, retain slightly higher entropy, indicating better\\npreservation of representational richness. Overall, these results reinforce that sustain-\\ning entropy diversity in early stages contributes to stronger representation learning in\\nVRAE.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='Fig. 3 Visualization of model performance trade-offs using the Pareto front. Each point denotes a\\nmodel evaluated by reconstruction quality (NMSE, PSNR, SSIM) versus inference speed (FPS), with\\ncolor indicating model size, i.e., the number of parameters. The red line represents the Pareto front,\\ni.e., the set of models that cannot be improved in one objective without sacrificing another. This\\nhighlights the optimal balance between accuracy and efficiency, helping to identify the most suitable\\nmodels depending on whether speed, quality, or a balanced trade-off is prioritized.\\n4.3 Comparative analysis of model trade-offs\\nTo evaluate the efficiency of different models in vehicle image restoration, we compare\\ntheir performance in terms of quality metrics (PSNR, SSIM, and NMSE) against\\ninference speed (FPS). Figure 3 shows the distribution of models with the Pareto\\nfront highlighted in red, while the color scale represents the number of parameters in\\nmillions.\\nIn the PSNRFPS and SSIMFPS plots, we observe that VRAE-based models\\n(e.g., VRAE3 and VRAE2) consistently lie on the Pareto front, striking a balance\\nbetween image quality and computational speed. In contrast, shallow autoencoders\\n(e.g., AE2 and AE3) achieve high FPS but suffer from significantly lower quality\\nscores, indicating that prioritizing speed alone comes at the cost of detail preservation.\\nMeanwhile, GAN and flow-based (FB) models demonstrate moderate performance,\\nbut their relatively high parameter counts make them less efficient.\\nThe NMSEFPS trade-off further highlights these limitations: although AE2\\nreaches the highest FPS, it exhibits considerably larger reconstruction error, while\\ndeeper VRAE variants (VRAE3, VRAE2) maintain low NMSE values with acceptable\\ninference speeds. These results underline the importance of designing models that pre-\\nserve sufficient depth of encoding to capture meaningful representations while avoiding\\nexcessive parameter growth.\\nOverall, the Pareto front demonstrates that VRAE architectures achieve a favor-\\nable trade-off across accuracy, speed, and parameter efficiency, making them strong\\ncandidates for real-time vehicle image restoration on edge devices.\\n4.4 Limitations\\nWhile the proposed VRAE models demonstrate superior reconstruction quality across\\nPSNR, NMSE, and SSIM, several limitations remain. As shown in Table 1, deeper\\nvariants (e.g., VRAE4 and VRAE5) yield consistent gains but incur substantially\\nhigher parameter counts and slower inference, revealing a clear trade-off between accu-\\nracy and efficiency. In addition, this work employs only base autoencoder and U-Net\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='architectures for clarity and accessibility, without exploring newer or more specialized\\ndesigns that could further enhance performance. Finally, the evaluation is conducted\\non a single dataset, which may limit the generalizability of the findings. Future work\\nwill address these aspects by extending to more efficient architectures and diverse\\ndatasets.\\n5 Conclusion and future work\\nVRAE was proposed the Variational Residual Autoencoder (VRAE) for vehicle image\\nrestoration, which combines an auxiliary encoder and feature aggregation to enhance\\nlocal detail and hierarchical representation. VRAE outperforms conventional Autoen-\\ncoders, GANs, and flow-based models in PSNR, SSIM, and NMSE, while offering\\ncompetitive inference speed. However, deeper VRAE variants improve quality at the\\ncost of higher complexity, limiting edge deployment. Future work includes explor-\\ning lightweight designs, compression methods, e.g., pruning, quantization, distillation,\\nand adaptive-depth architectures. VRAE can be further extended to real-time video\\nrestoration and other intelligent transportation tasks in future work.\\nReferences\\n[1] Pan, J., Sun, D., Pfister, H., Yang, M.-H.: Blind image deblurring using dark channel prior. In: Proceed-\\nings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16281636.\\nIEEE (2016)\\n[2] Kupyn, O., Budzan, V., Mykhailych, M., Mishkin, D., Matas, J.: Deblurgan: Blind motion deblurring\\nusing conditional adversarial networks. In: Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) Workshops, pp. 81838192. IEEE (2018)\\n[3] Nah, S., Kim, T.H., Lee, K.M.: Deep multi-scale convolutional neural network for dynamic scene\\ndeblurring. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 257265. IEEE (2017)\\n[4] Chen, C., Chen, Q., Xu, J., Koltun, V.: Learning to see in the dark. In: Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR) (2018). CVPR 2018\\n[5] Jiang, Y., Gong, X., Liu, D., Cheng, Y., Fang, C., Shen, X., Yang, J., Zhou, P.: Enlightengan: Deep\\nlight enhancement without paired supervision. arXiv preprint arXiv:1906.06972 (2019). arXiv 2019\\n[6] Zamir, S.W., Arora, A., Long, C., Sohail, A., Khan, S., Bhulla, M., Khan, F.S., Yang, M.-H., Khan,\\nM.A.: Learning enriched features for real image restoration and enhancement. In: European Conference\\non Computer Vision (ECCV) (2020). ECCV 2020\\n[7] Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of\\ndeep cnn for image denoising. IEEE Transactions on Image Processing 26(7), 31423155 (2017)\\n[8] Anwar, S., Barnes, N.: Real image denoising with feature attention. In: Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV) Workshops (2020). ICCV Workshops 2020\\n[9] Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767\\n(2018). arXiv 2018\\n[10] He, K., Gkioxari, G., Doll ar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE International\\nConference on Computer Vision (ICCV) (2017). ICCV 2017\\n[11] Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A.: Extracting and composing robust features\\nwith denoising autoencoders, 10961103 (2008). ACM\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/paper_3750.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[12] Kupyn, O., Martyniuk, T., Wu, J., Wang, Z.: Deblurgan-v2: Deblurring (orders-of-magnitude) faster\\nand better. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\\n88788887 (2019)\\n[13] Kingma, D.P., Dhariwal, P.: Glow: Generative flow with invertible 1x1 convolutions. In: Advances in\\nNeural Information Processing Systems, pp. 1021510224 (2018)\\n[14] Martin, S., Gagneux, A., Hagemann, P., Steidl, G.: Pnp-flow: Plug-and-play image restoration with\\nflow matching. International Conference on Learning Representations (ICLR) (2025)\\n[15] Zhu, Y., Zhao, W., Li, A., Tang, Y., Zhou, J., Lu, J.: Flowie: Efficient image enhancement via rectified\\nflow. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 1322 (2024)\\n[16] Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A.: Extracting and composing robust fea-\\ntures with denoising autoencoders. In: Proceedings of the 25th International Conference on Machine\\nLearning, pp. 10961103 (2008). ACM\\n[17] Xu, J., Li, Z., Guo, H., He, H.: Lightweight image super-resolution with information multi-distillation\\nnetwork. In: Proceedings of the 28th ACM International Conference on Multimedia, pp. 12371245\\n(2020)\\n[18] Jiang, K., Wang, Z., Yi, P., Wang, G., Lu, T., Jiang, J.: Deep distillation recursive network for remote\\nsensing image super-resolution. Remote Sensing 10(11), 1700 (2018)\\n[19] Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and new perspectives. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence 35(8), 17981828 (2013)\\n[20] Zhang, K., Zuo, W., Zhang, L.: Learning a single convolutional super-resolution network for multiple\\ndegradations. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 32623271 (2018)\\n[21] Ledig, C., Theis, L., Husz ar, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A., Tejani, A.,\\nTotz, J., Wang, Z., Shi, W.: Photo-realistic single image super-resolution using a generative adversarial\\nnetwork. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\n46814690 (2017)\\n[22] Wang, X., Yu, K., Dong, C., Loy, C.C.: Esrgan: Enhanced super-resolution generative adversarial\\nnetworks. Proceedings of the ECCV Workshops (2018)\\n[23] Zhang, K., Van Gool, L., Timofte, R.: Invertible image restoration. In: Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pp. 1008510094 (2021)\\n[24] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778 (2016)\\n[25] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks.\\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 47004708\\n(2017)\\n[26] Xu, Z., Yang, W., Meng, L., Lu, W., Huang, J., Zhou, J.: Towards end-to-end license plate detection\\nand recognition: A large dataset and baseline. European Conference on Computer Vision (ECCV)\\nWorkshops, 255271 (2018)\\n[27] Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversar-\\nial networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 11251134 (2017)\\n[28] Dinh, L., Sohl-Dickstein, J., Bengio, S.: Density estimation using real nvp. In: ICLR (2017)\\n[29] Meni, M.J., White, R.T., Mayo, M.L., Pilkiewicz, K.R.: Entropy-based guidance of deep neural net-\\nworks for accelerated convergence and improved performance. Information Sciences681, 121239 (2024)\\nhttps://doi.org/10.1016/j.ins.2024.121239\\n11')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b61b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x26fe8310a00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "def create_db_from_files(pdf_data_path, vector_db_path):\n",
    "    # Khai bao loader de quet toan bo thu muc dataa\n",
    "    loader = DirectoryLoader(pdf_data_path, glob=\"*.pdf\", loader_cls = PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=215, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Embeding\n",
    "    # embedding_model = GPT4AllEmbeddings(model_file=\"models/all-MiniLM-L6-v2-f16.gguf\")\n",
    "    db = FAISS.from_documents(chunks, embedding_model)\n",
    "    db.save_local(vector_db_path)\n",
    "    return db\n",
    "\n",
    "pdf_data_path = \"data\"\n",
    "vector_db_path = \"vectorstores\"\n",
    "create_db_from_files(pdf_data_path, vector_db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_db = FAISS.load_local(\n",
    "    vector_db_path,\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "query = \"how many dataset are mentioned in the document?\"\n",
    "results = faiss_db.similarity_search(query, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d4ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f51fc8f4-c25b-4c6b-b0d5-cb80592f769f', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\paper_3750.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='on a single dataset, which may limit the generalizability of the findings. Future work\\nwill address these aspects by extending to more efficient architectures and diverse\\ndatasets.\\n5 Conclusion and future work'),\n",
       " Document(id='fb417626-391d-4160-a068-bcdd0613fb19', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\paper_3750.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='3 Experiments\\n3.1 Dataset Preparation\\nFrom the CCPD dataset [26], 3,036 high-resolution vehicle images were collected and\\nresized to 256256 pixels. The dataset was split into training, validation, and test sets'),\n",
       " Document(id='a96f121d-3484-435f-a504-a528576b55f9', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\paper_3750.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='in a 70%15%15% ratio. Data augmentation, including rotations, was applied only to\\nthe training set, increasing it to 7,000 images. Low-resolution inputs were generated'),\n",
       " Document(id='5ac63086-2d25-4d46-b2c3-cf32f571452d', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\paper_3750.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='(2017)\\n[26] Xu, Z., Yang, W., Meng, L., Lu, W., Huang, J., Zhou, J.: Towards end-to-end license plate detection\\nand recognition: A large dataset and baseline. European Conference on Computer Vision (ECCV)'),\n",
       " Document(id='89d6056a-5e9b-4900-a114-8729dd1edaa1', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\paper_3750.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='performance across different depths but also surpasses conventional baselines in both\\nreconstruction fidelity and computational efficiency.\\n4.2 Informative content preservation'),\n",
       " Document(id='73d54eef-5ce6-4af6-8028-139aa53ef069', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\paper_3750.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='the representational diversity of feature maps. A positive Hl implies increased repre-\\nsentational diversity, while a negative Hl indicates information compression. In this'),\n",
       " Document(id='bdb60842-ef06-4f0a-9fce-cebdfce1cda4', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\paper_3750.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='all samples. The best results are in bold, and the second-best are underlined.\\nModel PSNR ( ) NMSE ( ) SSIM ( ) Params FPS\\nGAN (Gen) 28.743 0.006 0.842 14.59M 188\\nFB (Gen) 27.093 0.008 0.831 25.87M 35'),\n",
       " Document(id='04233f1d-712f-43de-a621-3b1495416976', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\paper_3750.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='4.2 Informative content preservation\\nAccording to [29], encouraging entropy preservation in the early layers promotes better\\ngeneralization. This is consistent with our hypothesis Figure 4.2, where we observe'),\n",
       " Document(id='1ea2128d-9347-40cc-9bed-26f28982a738', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\paper_3750.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[25] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks.\\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 47004708\\n(2017)'),\n",
       " Document(id='a0e2ba2f-2433-4573-a08b-d439553e1231', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-29T06:21:35+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-29T06:21:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data\\\\paper_3750.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='International Conference on Computer Vision (ICCV) Workshops (2020). ICCV Workshops 2020\\n[9] Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767\\n(2018). arXiv 2018')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc8a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION: You are helpful assistant that answers questions based on provided context.\n",
      "[1] Source: data\\paper_3750.pdf\n",
      "on a single dataset, which may limit the generalizability of the findings. Future work will address these aspects by extending to more efficient architectures and diverse datasets. 5 Conclusion and future work\n",
      "\n",
      "---\n",
      "\n",
      "[2] Source: data\\paper_3750.pdf\n",
      "3 Experiments 3.1 Dataset Preparation From the CCPD dataset [26], 3,036 high-resolution vehicle images were collected and resized to 256256 pixels. The dataset was split into training, validation, and test sets\n",
      "\n",
      "---\n",
      "\n",
      "[3] Source: data\\paper_3750.pdf\n",
      "in a 70%15%15% ratio. Data augmentation, including rotations, was applied only to the training set, increasing it to 7,000 images. Low-resolution inputs were generated\n",
      "\n",
      "---\n",
      "\n",
      "[4] Source: data\\paper_3750.pdf\n",
      "(2017) [26] Xu, Z., Yang, W., Meng, L., Lu, W., Huang, J., Zhou, J.: Towards end-to-end license plate detection and recognition: A large dataset and baseline. European Conference on Computer Vision (ECCV)\n",
      "\n",
      "---\n",
      "\n",
      "[5] Source: data\\paper_3750.pdf\n",
      "performance across different depths but also surpasses conventional baselines in both reconstruction fidelity and computational efficiency. 4.2 Informative content preservation\n",
      "\n",
      "---\n",
      "\n",
      "[6] Source: data\\paper_3750.pdf\n",
      "the representational diversity of feature maps. A positive Hl implies increased repre- sentational diversity, while a negative Hl indicates information compression. In this\n",
      "\n",
      "---\n",
      "\n",
      "[7] Source: data\\paper_3750.pdf\n",
      "all samples. The best results are in bold, and the second-best are underlined. Model PSNR ( ) NMSE ( ) SSIM ( ) Params FPS GAN (Gen) 28.743 0.006 0.842 14.59M 188 FB (Gen) 27.093 0.008 0.831 25.87M 35\n",
      "\n",
      "---\n",
      "\n",
      "[8] Source: data\\paper_3750.pdf\n",
      "4.2 Informative content preservation According to [29], encouraging entropy preservation in the early layers promotes better generalization. This is consistent with our hypothesis Figure 4.2, where we observe\n",
      "\n",
      "---\n",
      "\n",
      "[9] Source: data\\paper_3750.pdf\n",
      "[25] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 47004708 (2017)\n",
      "\n",
      "---\n",
      "\n",
      "[10] Source: data\\paper_3750.pdf\n",
      "International Conference on Computer Vision (ICCV) Workshops (2020). ICCV Workshops 2020 [9] Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767 (2018). arXiv 2018\n",
      "Answer the question: how many dataset are mentioned in the document?\n",
      " \n",
      "\n",
      " RESPONSE: \n",
      "There are 9 datasets mentioned in the document.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract respond using LLM model\n",
    "\n",
    "question = query  \n",
    "\n",
    "# Build short snippets (truncate to avoid token overflow)\n",
    "snippets = []\n",
    "for i, doc in enumerate(results):\n",
    "    src = doc.metadata.get(\"source\", f\"doc_{i+1}\")\n",
    "    text = \" \".join(doc.page_content.split())  # collapse whitespace\n",
    "    snippets.append(f\"[{i+1}] Source: {src}\\n{text}\")\n",
    "\n",
    "context = \"\\n\\n---\\n\\n\".join(snippets)\n",
    "\n",
    "prompt = f\"\"\"You are helpful assistant that answers questions based on provided context.\n",
    "{context}\n",
    "Answer the question: {question}\n",
    "\"\"\"\n",
    "promt_template = PromptTemplate.from_template(\"\"\"INSTRUCTION: {prompt} \\n\\n RESPONSE:\"\"\")\n",
    "\n",
    "prompt = promt_template.format(prompt=prompt)\n",
    "answer = llm.invoke(prompt)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
